<h1 class="itvmo-article-header-blue no-top">3. Generative AI Procurement</h1>

<h2 class="itvmo-article-sub-2 no-top">3.4 Data Management and Protection</h2>

<p>
  Data is at the heart of all Generative AI tools. The foundation models are
  trained on billions to trillions of pieces of data. Users can put new data in
  and the models can learn from that data. Generative AI tools produce text,
  images and a whole host of other data. Properly protecting and managing that
  data as required by applicable law and policy is a critical consideration for
  any Generative AI acquisition.
</p>

<p>
  It’s critical to know what data was used to build and train the system
  initially and how the training data relates to the data you will use the
  Generative AI tool on, including potential implications for bias. It’s also
  more important to know whether the data you input will be used to train and
  improve the system or whether it will be deleted after the query is complete.
  You’ll need to know how the data you input might be outputted to other users
  when responding to their prompts. And you will also want to understand how you
  can transfer or use your data in other systems. The IPT can then identify,
  document and monitor where there are unknowns around the data outputs, usage
  and ownership. The agency’s Chief Data Officer (CDO) could be able to answer
  specific questions and concerns about data rights and ownership.
</p>

<h3 class="itvmo-article-sub-3-blue">3.4.1 Data Security</h3>
<p>
  Using data responsibly and protecting data from unauthorized access are high
  priorities for data from both inside and outside of the government. Working
  with agency officials responsible for security, privacy and other areas of
  data governance is important for understanding what laws, regulations and
  policies apply to any data that will be involved in agency use of Generative
  AI; for assessing associated risks; and for implementing appropriate
  safeguards.
</p>

<p>
  Protections can help prevent unauthorized access. If there is any chance
  information considered non-public by your agency officials would be used in a
  Generative AI tool, then those tools can only run on systems that have the
  right protections and safeguards in place. But even with publicly available
  information or information available for unlimited distribution there may be
  applicable laws, regulations, policies and standards.
</p>

<h3 class="itvmo-article-sub-3-blue">3.4.2 Data Inventory</h3>

<p>
  <b>
    Listing the data that will be involved in the Generative AI tools the agency
    plans to use can help manage the data. Major sources of data to manage
    include but aren’t limited to:
  </b>
</p>
<div class="bulletSections">
  <div class="topLine"></div>
  <ul class="experiences">
    <li class="bulletText tight-bullet">
      <b>Generative AI Training, Testing and Validation Data</b> - This is the
      information that the manufacturer of the Generative AI tool used to create
      the LLM and see if it worked well.
    </li>
    <li class="bulletText tight-bullet">
      <b>Generative AI Fine-Tuning Data</b> - This is the information that
      either the manufacturer of the Generative AI tool or the users of the tool
      (including your agency) use to customize the LLM to better serve specific
      purposes.
    </li>
    <li class="bulletText tight-bullet">
      <b>Agency Inputted Data</b> - This is the information that the agency
      staff feeds into the model so the LLM can summarize it, find patterns and
      insights or transform it and the prompts that agencies put in to get
      answers from the LLM.
    </li>
    <li class="bulletText tight-bullet">
      <b>Generative AI Outputted Data</b> - This is the information that the LLM
      creates based on the training, testing and agency inputted data.
    </li>
    <li class="bulletText tight-bullet">
      <b>Generative AI Code and Models</b> - This is the set of algorithms and
      programming that make up the model itself.
    </li>
  </ul>
</div>

<h3 class="itvmo-article-sub-3-blue">3.4.3 Understanding How Data is Used</h3>
<p>
  Understanding and identifying training and testing data, input data and output
  data are critical to managing a successful Generative AI project or program.
  Both users and owners of the tool and data have equities in how they are
  managed and used.
</p>

<p>
  Generative AI’s need for training data raises questions about where, when and
  how that data was collected. Model cards and similar resources can help
  explain the origin and nature of the training data and quantify the risk of
  bias, discrimination or similar harm to people. Understanding the data, its
  biases and its current applications and usage will help to identify whether a
  particular application or model is more appropriate to meet needs.
</p>

<p>
  Asking suppliers to share information about their products helps acquisition
  staff know that there is transparency and accountability in the procurement.
  At the same time, it helps the IPT, CAIO and other relevant agency officials
  monitor compliance later.
</p>

<p>
  Here are initial questions the IPT can ask to help get a better sense of what
  data are associated with the agency’s use of Generative AI, how to work with
  relevant agency officials to assess and mitigate risks, how commercial
  products were developed, what actions suppliers have taken and how to ensure
  government data rights are protected.
</p>

<p>
  <b>
    This list of questions is not exhaustive, nor will all the questions be
    applicable in every situation.
  </b>
</p>
<div class="bulletSections">
  <div class="topLine"></div>
  <ul class="experiences">
    <li class="bulletText tight-bullet">
      What steps have you taken to ensure the data are suitable for the intended
      use case?
    </li>
    <li class="bulletText tight-bullet">
      What steps have you taken to evaluate the provenance and quality of the
      data?
    </li>
    <li class="bulletText tight-bullet">
      What is the source of the data (e.g., agency data, social media, news,
      legal documents, scientific reports, etc.)?
    </li>
    <li class="bulletText tight-bullet">
      Have you worked with relevant agency officials to evaluate the security
      risks, privacy risks, and other data governance issues associated with the
      data (including the training data), the model, and the intended use case?
      Have you worked with relevant agency officials to implement appropriate
      safeguards?
    </li>
    <li class="bulletText tight-bullet">
      What data were used for training and testing? Have you considered, for
      example, the size of the data set and the demographics associated with the
      data set? Have any inherent biases been identified in the data? What
      biases are likely present in the data?
    </li>
    <li class="bulletText tight-bullet">
      Was specific data used to “fine tune” the model? If so, what were the
      characteristics of those data? What steps have you taken to ensure these
      data are suitable for the intended use case?
    </li>
    <li class="bulletText tight-bullet">
      For training/testing data and agency inputted data, what was the context
      in which the data set was gathered or generated? What was its initial
      intended purpose(s)?
    </li>
    <li class="bulletText tight-bullet">
      For agency inputted data, are agency inputs stored by vendors? If so, what
      are the limitations on how vendors use that data?
    </li>
    <li class="bulletText tight-bullet">
      For Generative AI outputted data, who owns the data after they are
      generated/modiﬁed?
    </li>
    <li class="bulletText tight-bullet">
      Will Generative AI outputted data be used to train another AI model you
      are using?
    </li>
    <li class="bulletText tight-bullet">
      Could the intended use case have an impact on rights or safety?
    </li>
    <li class="bulletText tight-bullet">
      Does the agency have the ability to assess, monitor, and control outputs?
    </li>
    <li class="bulletText tight-bullet">Can the outputs be explained?</li>
    <li class="bulletText tight-bullet">
      Are there any risks of data leakage?
    </li>
    <li class="bulletText tight-bullet">
      Who owns the model once the contract ends?
    </li>
    <li class="bulletText tight-bullet">
      Does the model have any sort of history of bias or ethical challenges that
      have not been corrected?
    </li>
    <li class="bulletText tight-bullet">
      Could use of the model inadvertently synthesize information in a way that
      makes the output more sensitive than the input?
    </li>
    <li class="bulletText tight-bullet">
      What kinds of protections or submodels are being formed with data that
      could inadvertently lead to adverse outcomes for government customers?
    </li>
  </ul>
</div>

<h3 class="itvmo-article-sub-3-blue">3.4.4 Potential Data-Related Risks</h3>
<p>
  This resource guide lists some but not all of the potential risks of
  Generative AI in
  <a
    class="usa-link"
    rel="noreferrer noopener"
    href="{{site.baseurl}}/genai/?tabName=part-1-header"
    >Section 1.4 Potential Risks of Generative AI</a
  >
  including Misinformation and Disinformation, Privacy, Bias and Discrimination
  and Security/Cybersecurity. Clarifying ethical challenges related to data and
  completing a risk assessment are potential ways to identify risks for data and
  determine how to monitor and mitigate them.
</p>
<div class="indent-article-section-1">
  <h4 class="itvmo-article-sub-4">Clarify ethical challenges</h4>
  <p>
    Documents like the
    <a
      class="usa-link usa-link--external"
      href="https://www.whitehouse.gov/ostp/ai-bill-of-rights/"
      >Blueprint for an AI Bill of Rights</a
    >
    and the
    <a class="usa-link usa-link--external" href="https://rai.tradewindai.com/"
      >Tradewinds Responsible AI Toolkit</a
    >
    are published ethical frameworks that help explain what issues to
    prioritize.
  </p>

  <p class="mb-0"><b>From the Responsible AI website:</b></p>

  <p class="article-quote">
    “The Responsible Artificial Intelligence (RAI) Toolkit provides a
    centralized process that identifies, tracks, and improves alignment of AI
    projects to RAI best practices and the DoD AI Ethical Principles, while
    capitalizing on opportunities for innovation. The RAI Toolkit provides an
    intuitive flow guiding the user through tailorable and modular assessments,
    tools, and artifacts throughout the AI product lifecycle. The process
    enables traceability and assurance of responsible AI practice, development,
    and use.”
  </p>

  <h4 class="itvmo-article-sub-4">Complete a risk assessment</h4>

  <p>
    For each of the areas listed above, capturing both what might go wrong and
    the potential harm, damage or loss that the agency would suffer if that
    happened can be helpful for determining how data are used.
  </p>

  <blockquote class="border-left-05 border-primary-darker padding-left-2">
    "The Responsible AI Toolkit also provides an example guide on risk.
    <br />
    <a
      class="usa-link usa-link--external"
      href="https://rai.tradewindai.com/appendix/dagr">
      Responsible Artificial Intelligence (RAI) Defense AI Guide on Risk
      (DAGR)</a
    >
    is intended to provide DoD AI stakeholders with a voluntary guide to promote
    holistic risk captures and improved trustworthiness, effectiveness,
    responsibility, risk mitigation, and operations.”
  </blockquote>

  <p>
    The
    <a
      class="usa-link usa-link--external"
      href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf"
      >NIST Risk Management Framework (RMF)</a
    >
    helps to better manage risks to individuals, organizations, and society
    associated with artificial intelligence (AI) by incorporating
    trustworthiness considerations into the design, development, use, and
    evaluation of AI products, services, and systems. The accompanying
    <a
      class="usa-link usa-link--external"
      href="https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook"
      >NIST AI RMF Playbook</a
    >
    provides suggested actions for achieving the outcomes laid out in the AI
    Risk Management Framework (AI RMF). Suggestions are aligned to each
    sub-category within the four AI RMF functions (Govern, Map, Measure,
    Manage). The Playbook is neither a checklist nor set of steps to be followed
    in its entirety. Playbook suggestions are voluntary. Organizations may
    utilize this information by borrowing as many – or as few – suggestions as
    apply to their industry use case or interests.
  </p>

  <p>
    If any data was used for training, tuning, validating or fine-tuning the
    model, IPTs are encouraged to persist the data for contestability or
    observability of issues like data drift.
  </p>
</div>
<div class="tab-divide-line w-100"></div>
