<h2>3.4 Data Management and Protection</h2>

<p>
  Data is at the heart of all Generative AI tools. The foundation models are
  trained on billions to trillions of pieces of data. Users can put new data in
  and the models will learn from that data. And Generative AI tools produce
  text, images and a whole host of other data. Properly protecting and managing
  that data as required by applicable law and policy is a critical consideration
  for any Generative AI acquisition.
</p>

<p>
  It’s critical to know what data was used to build and train the system
  initially and how the training data relates to the data you will use the
  Generative AI tool on, including potential implications for bias. It’s equally
  more important to know whether the data you input will be used to train and
  improve the system or whether it will be deleted after the query is complete.
  You’ll need to know how the data you input might be outputted to other users
  when responding to their prompts. And you will also want to understand how you
  can transfer to or use your data in other systems. The IPT can then identify,
  document and monitor where there are unknowns around the data outputs, usage
  and ownership.
</p>

<p>
  <a
    class="usa-link usa-link--external"
    href="https://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk-Management-for-Agency-Use-of-Artificial-Intelligence.pdf"
    >OMB Management Memo M-24-10</a
  >
  has requirements for what data to review and issues to investigate.
</p>

<h3>3.4.1 Data Security</h3>
<p>
  Using data responsibly and protecting data from unauthorized access are high
  priorities for data from both inside or outside of the government. Consult
  with your Agency’s Privacy Officer, CIO and Security Officer to assess data
  management requirements and risks. At the most basic level, data must be
  securely stored and securely transmitted. Secure networks prevent data from
  being compromised in transit.
</p>

<p>
  Protections prevent unauthorized access. If there is any chance information
  considered non-public by your agency officials would be used in a Generative
  AI tool, then those tools can only run on systems that have the right
  protections and safeguards in place. But even with publicly available
  information or information available for unlimited distribution, ensure that
  Federal data protection standards and regulations will be met. And because
  cyber threats are only getting worse, Generative AI systems must have proper
  cybersecurity systems including threat detection and risk mitigation.
</p>

<h3>3.4.2 Data Inventory</h3>
<p>
  List the data that will be involved in the Generative AI tools the agency
  plans to use. Major types of data to manage include but isn’t limited to:
</p>

<ul>
  <li>
    Generative AI Training, Testing and Validation Data - This is the
    information that the manufacturer of the Generative AI tool used to create
    the LLM and see if it worked well.
  </li>

  <li>
    Generative AI Fine-Tuning Data - This is the information that either the
    manufacturer of the Generative AI tool or the users of the tool (including
    your agency) use to customize the LLM to better serve specific purposes.
  </li>

  <li>
    Agency Inputted Data - This is the information that the agency staff feeds
    into the model so the LLM can summarize it, find patterns and insights or
    transform it and the prompts that agencies put in to get answers from the
    LLM.
  </li>

  <li>
    Generative AI Outputted Data - This is the information that the LLM creates
    based on the training, testing and agency inputted data.
  </li>

  <li>
    Gen AI Code and Models - This is the set of algorithms and programming that
    make up the model itself.
  </li>
</ul>

<h3>3.4.3 Understanding How Data is Used</h3>
<p>
  Understanding and identifying training and testing data, input data and output
  data are critical to managing a successful Generative AI project or program.
  Both users and owners of the tool and data have equities in how they are
  managed and used.
</p>

<p>
  Here are initial questions to help the IPT get a better sense of the data
  associated with the agency’s use of Generative AI, how to appropriately assess
  and mitigate risks, and how to ensure government data rights are protected.
  This list of questions is not exhaustive, nor will all the questions be
  applicable in every situation.
</p>

<h4>Generative AI Training, Testing and Validation Data</h4>
<ul>
  <li>What data were used for training, testing and validation?</li>
  <li>What is the size of the data set?</li>
  <li>
    Is there a discernible geographic, age or other demographic with the data?
  </li>
  <li>
    What is the source of the data (e.g., agency data, social media, news, legal
    documents, scientific reports, etc.)?
  </li>
  <li>
    What was the context in which the data set was gathered or generated and
    what was its initial intended purpose(s)?
  </li>
  <li>Have any inherent biases been identified in the data?</li>
  <li>What biases are likely present in the data?</li>
  <li>
    Are there allegations of copyright infringement or pending court cases which
    might impact the data set and tool?
  </li>
</ul>
<ul>
  <li>
    Have there been any studies on the data that may indicate prior use and
    intent?
  </li>
</ul>

<h4>Agency Inputted Data</h4>
<ul>
  <li>Who generated the data?</li>
  <li>
    What was the context in which the data set was gathered or generated and
    what was its initial intended purpose(s)?
  </li>
  <li>
    Are agency inputs stored by vendors? If so, what are the limitations on how
    vendors use that data?
  </li>
  <li>
    How will the data you input be used to train the system and make it better?
  </li>
  <li>
    How might the data you input be shared with other users when responding to
    their prompts?
  </li>
</ul>

<h4>Generative AI Outputted Data</h4>
<ul>
  <li>Who owns the IP generated by AI tools (e.g. software code)?</li>
  <li>Who owns the model once the contract ends?</li>
  <li>
    Will outputted data be used to train the Generative AI model you are using
    or another model with or without your knowledge?
  </li>
  <li>Could outputted data impact citizen entitlements, benefits or rights?</li>
  <li>
    Does the agency have the ability to assess, monitor and control outputs?
  </li>
  <li>What kinds of outputs does the system filter out?</li>
  <li>Who owns the outputs from the tool?</li>
  <li>
    Does the agency have the ability to assess, monitor, and control outputs?
  </li>
  <li>Can the outputs be explained?</li>
</ul>

<h4>Generative AI Model</h4>
<ul>
  <li>Will you be able to input your own data to get better results?</li>
  <li>How have other users’ data been incorporated back into the model?</li>
  <li>When was the last time the model was updated and what was changed?</li>
  <li>
    Was specific data used to “fine tune” the model? If so, what were the
    characteristics of those data?
  </li>
  <li>
    What steps have you taken to ensure the data used for “fine tuning” are
    suitable for the intended use case?
  </li>
  <li>
    Will agency data be used to train the Generative AI model you are using?
  </li>
  <li>Who owns the model once the contract ends?</li>
  <li>
    Does the model have any sort of history of bias or ethical challenges that
    have not been corrected?
  </li>
  <li>Are there any risks of data leakage?</li>
  <li>
    Could use of the model inadvertently synthesize information in a way that
    makes the output more sensitive than the input?
  </li>
  <li>
    Could use of the model inadvertently synthesize sensitive or CUI information
    or information perceived to be sensitive or CUI?
  </li>
  <li>
    What kinds of protections or submodels are being formed with data that could
    inadvertently lead to adverse outcomes for government customers
  </li>
</ul>

<p>
  Generative AI’s need for training data raises questions and privacy concerns
  about where, when and how that data was collected. Model cards and similar
  resources can help explain the origin and nature of the training data and
  quantify the risk of bias, discrimination or similar harm to people.
</p>

<p>
  Generative AI applications are built on training data. Understanding the data,
  its biases and its current applications and usage will help to identify
  whether a particular application or model is more appropriate to meet needs.
</p>

<p>
  The CAIO of the agency is responsible for ensuring that Generative AI systems
  comply with relevant standards as detailed in
  <a
    class="usa-link usa-link--external"
    href="https://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk-Management-for-Agency-Use-of-Artificial-Intelligence.pdf"
    >OMB Management Memo M-24-10</a
  >. Acquisition staff can assist by documenting their process and decisions.
</p>

<p>
  Asking suppliers to share information about their products helps acquisition
  staff know that there is transparency and accountability in the procurement.
  At the same time, it helps the IPT and CAIO jointly monitor compliance later.
</p>

<p>
  One example of a “declaration form” for a Generative AI tool was created by
  the GovAI Coalition, a group of public servants from over 150 local, county,
  and state governments. See their
  <a
    class="usa-link usa-link--external"
    href="https://www.sanjoseca.gov/home/showpublisheddocument/109730/638458752830000000"
    >AI Fact Sheet for Third Party Systems</a
  >.
</p>

<h3>3.4.4 Potential Data-Related Risks</h3>
<p>
  This guide lists some but not all of the potential risks of Generative AI in
  Section 1 including Misinformation and Disinformation, Privacy, Bias and
  Discrimination and Security/Cybersecurity. Clarifying ethical challenges
  related to data and completing a risk assessment are potential ways to
  identify risks for data and determine how to monitor and mitigate them.
</p>

<h4>Clarify ethical challenges.</h4>
<p>
  Documents like the
  <a
    class="usa-link usa-link--external"
    href="https://www.whitehouse.gov/ostp/ai-bill-of-rights/"
    >Blueprint for an AI Bill of Rights</a
  >
  and the
  <a class="usa-link usa-link--external" href="https://rai.tradewindai.com/"
    >Tradewinds Responsible AI Toolkit</a
  >
  are published ethical frameworks that help explain what issues to prioritize.
</p>

<p>From the Responsible AI website:</p>

<p>
  “The Responsible Artificial Intelligence (RAI) Toolkit provides a centralized
  process that identifies, tracks, and improves alignment of AI projects to RAI
  best practices and the DoD AI Ethical Principles, while capitalizing on
  opportunities for innovation. The RAI Toolkit provides an intuitive flow
  guiding the user through tailorable and modular assessments, tools, and
  artifacts throughout the AI product lifecycle. The process enables
  traceability and assurance of responsible AI practice, development, and use.”
</p>

<h4>Complete a risk assessment.</h4>
<p>
  For each of the areas listed above, the IPT can capture both what might go
  wrong and the potential harm, damage or loss that the agency would suffer if
  that happened.
</p>

<p>
  The Responsible AI Toolkit also has a guide on risk. “The
  <a
    class="usa-link usa-link--external"
    href="https://rai.tradewindai.com/appendix/dagr"
    >Responsible Artificial Intelligence (RAI) Defense AI Guide on Risk
    (DAGR)</a
  >
  is intended to provide DoD AI stakeholders with a voluntary guide to promote
  holistic risk captures and improved trustworthiness, effectiveness,
  responsibility, risk mitigation, and operations.” It is also a valuable
  resource for non-DoD agencies.
</p>

<p>
  The
  <a
    class="usa-link usa-link--external"
    href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf"
    >NIST Risk Management Framework (RMF)</a
  >
  helps to better manage risks to individuals, organizations, and society
  associated with artificial intelligence (AI) by incorporating trustworthiness
  considerations into the design, development, use, and evaluation of AI
  products, services, and systems. The accompanying
  <a
    class="usa-link usa-link--external"
    href="https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook"
    >NIST AI RMF Playbook</a
  >
  provides suggested actions for achieving the outcomes laid out in the AI Risk
  Management Framework (AI RMF). Suggestions are aligned to each sub-category
  within the four AI RMF functions (Govern, Map, Measure, Manage). The Playbook
  is neither a checklist nor set of steps to be followed in its entirety.
  Playbook suggestions are voluntary. Organizations may utilize this information
  by borrowing as many – or as few – suggestions as apply to their industry use
  case or interests.
</p>

<div class="tab-divide-line w-100"></div>
