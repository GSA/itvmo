<h2 class="ext-top itvmo-article-sub-2">
  1.2 A Basic Overview of Generative AI
</h2>

<p>
  Here is a simplified crash course into Generative Artificial Intelligence or
  “Generative AI.”
</p>
<div class="indent-article-section-1">
  <h3 class="itvmo-article-sub-3-blue">
    Predictive Text is a Simple Example of Artificial Intelligence
  </h3>
  <p>
    We’ve all seen the basic version of artificial intelligence in computers and
    smartphones.
  </p>

  <p>
    When we are writing an email or composing a text message the software
    suggests what the next word we might want to type is. That “typeahead”
    feature is an example of basic artificial intelligence in action.
  </p>

  <p>
    Predictive text relies on the most fundamental building block of Generative
    AI tools, called “foundation models.” Simply put these models are pieces of
    software that identify patterns in large quantities of data to predict the
    next item in a sequence.
  </p>

  <p>
    Predictive text uses one of these foundation models to predict your next
    word based on what most people have written and what you’ve written in other
    sentences.
  </p>

  <h3 class="itvmo-article-sub-3-blue">The Rise of Large Language Models</h3>
  <p>We can now go much farther than simple predictive text.</p>

  <p>
    That’s because in the last few years we’ve created more sophisticated
    artificial intelligence models with immensely more data and more powerful
    computers. These new models have learned how we construct sentences,
    paragraphs and longer pieces of writing.
  </p>

  <p>
    The creators of these models gathered trillions of pieces of written
    language, from encyclopedia entries and literature to social media posts and
    song lyrics and even the spoken word data and imagery of videos shared
    online. They used all that data to “train” their models.
  </p>

  <p>
    Those humans wrote computer code and “algorithms” that looked at every word
    and phrase and analyzed how they connect to one another: What do those words
    mean? Which words tend to show up near others? What words are used in which
    contexts?
  </p>

  <p>
    The algorithms looked for patterns and statistics in the interrelationships
    of words and phrases. And the software kept track of the results.
  </p>

  <p>
    All of that together created a powerful kind of foundation model called a
    “Large Language Model” or LLM.
  </p>

  <h3 class="itvmo-article-sub-3-blue">
    Artificial Intelligence that Generates Content
  </h3>
  <p>
    The human software programmers also figured out how to build those LLMs to
    get really good at responding to simple human questions and prompts. The
    LLMs could interpret the language a person inputted, analyze what it should
    do and provide a response.
  </p>

  <p>
    The programmers built chat interfaces similar to text messages that allow
    someone to write a simple prompt or question and have the tool respond.
  </p>

  <p>
    The tool wouldn’t just search all the trillions of pieces of written
    language information in its memory and serve up potential answers. It would
    use its pattern analysis of language to compose new phrases and sentences
    and give an answer. The artificial intelligence system would generate
    content; hence, Generative AI.
  </p>

  <p>
    Another benefit of the chat interface has been that someone could ask the
    system to improve the output in specific ways like:
  </p>
  <ul class="basic-list">
    <li>make the tone more friendly;</li>
    <li>use fewer bullets; or</li>
    <li>
      say more about this specific issue referencing the events of the last few
      months.
    </li>
  </ul>

  <p>
    In this way, humans can keep shaping and refining the output from Generative
    AI systems. And after a few rounds of refinement, we can get something very
    close to what we want.
  </p>

  <p>
    <b>
      (Note: Large Language Models nowadays are often called Generative
      Pre-Trained Transformers or GPTs. Pairing a “Chat” function with a “GPT”
      is what gave the popular Generative AI product its name. While GPTs and
      LLMs are technically different, you can think of them as about the same
      for the purpose of Generative AI acquisition.)
    </b>
  </p>

  <h3 class="itvmo-article-sub-3-blue">Beyond Text</h3>
  <p>
    From there companies and researchers started creating other kinds of
    foundation models and teaching them to learn how to analyze and find
    patterns in different inputs.
  </p>

  <p>
    People started feeding the new models photographs and paintings, songs and
    speeches, videos and animation, computer code and numerical data. The models
    found patterns in these kinds of content as well.
  </p>

  <p>
    With a similar chat-based interface it became pretty easy to now ask for
    more than a written text response. Now these foundation models could
    generate images, music, movies, and smartphone apps.
  </p>

  <p>
    As time goes on and models get better, the ability of these artificial
    intelligence tools to generate more sophisticated content will only
    increase.
  </p>
</div>
